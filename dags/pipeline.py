"""
DAG Principal del Pipeline ETL Financiero
Extrae datos de 3 APIs, transforma y prepara para carga
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.empty import EmptyOperator
import logging

# Importar nuestros m√≥dulos
import sys
sys.path.insert(0, '/opt/airflow')

from src.extractor import extraer_todas_las_fuentes
from src.transformer import transformar_datos_completo

# Configurar logging
logger = logging.getLogger(__name__)


# ============================================
# FUNCIONES PARA LAS TAREAS
# ============================================

def tarea_extraer_datos(**context):
    """
    Tarea 1: Extrae datos de las 3 APIs
    """
    logger.info("üöÄ Iniciando extracci√≥n de datos...")
    
    try:
        # Extraer datos (1 d√≠a para ejecuci√≥n diaria)
        datos = extraer_todas_las_fuentes(dias_historico=1)
        
        # Log de resumen
        logger.info(f"‚úÖ Extracci√≥n completada:")
        logger.info(f"  ‚Ä¢ Productos: {len(datos['productos'])} registros")
        logger.info(f"  ‚Ä¢ Tipos de cambio: {len(datos['tipos_cambio'])} registros")
        logger.info(f"  ‚Ä¢ Adicionales: {len(datos['adicionales'])} registros")
        
        # Guardar en XCom para la siguiente tarea
        context['ti'].xcom_push(key='datos_extraidos', value={
            'productos_count': len(datos['productos']),
            'tipos_cambio_count': len(datos['tipos_cambio']),
            'adicionales_count': len(datos['adicionales'])
        })
        
        # Retornar los DataFrames para la siguiente tarea
        return datos
        
    except Exception as e:
        logger.error(f"‚ùå Error en extracci√≥n: {str(e)}")
        raise


def tarea_transformar_datos(**context):
    """
    Tarea 2: Transforma y consolida los datos
    """
    logger.info("üîÑ Iniciando transformaci√≥n de datos...")
    
    try:
        # Obtener datos de la tarea anterior
        datos = context['ti'].xcom_pull(task_ids='extraer_datos')
        
        if not datos:
            raise ValueError("No se recibieron datos de la tarea de extracci√≥n")
        
        # Transformar datos
        df_final, resumen = transformar_datos_completo(
            datos['productos'],
            datos['tipos_cambio'],
            datos['adicionales'],
            moneda_local='ARS'
        )
        
        # Log de resumen
        logger.info(f"‚úÖ Transformaci√≥n completada:")
        logger.info(f"  ‚Ä¢ Total registros: {len(df_final)}")
        logger.info(f"  ‚Ä¢ Columnas: {len(df_final.columns)}")
        
        # Guardar resumen en XCom
        context['ti'].xcom_push(key='datos_transformados', value={
            'registros_totales': len(df_final),
            'columnas_totales': len(df_final.columns),
            'resumen': resumen
        })
        
        logger.info("üìä Datos listos para carga (pr√≥ximamente)")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error en transformaci√≥n: {str(e)}")
        raise


def tarea_resumen_final(**context):
    """
    Tarea final: Muestra resumen de la ejecuci√≥n
    """
    logger.info("üìã Generando resumen final...")
    
    try:
        # Obtener datos de XCom
        datos_extraidos = context['ti'].xcom_pull(
            task_ids='extraer_datos',
            key='datos_extraidos'
        )
        
        datos_transformados = context['ti'].xcom_pull(
            task_ids='transformar_datos',
            key='datos_transformados'
        )
        
        # Mostrar resumen
        logger.info("=" * 60)
        logger.info("‚úÖ PIPELINE EJECUTADO EXITOSAMENTE")
        logger.info("=" * 60)
        
        if datos_extraidos:
            logger.info("üì• EXTRACCI√ìN:")
            logger.info(f"  ‚Ä¢ Productos: {datos_extraidos['productos_count']}")
            logger.info(f"  ‚Ä¢ Tipos de cambio: {datos_extraidos['tipos_cambio_count']}")
            logger.info(f"  ‚Ä¢ Adicionales: {datos_extraidos['adicionales_count']}")
        
        if datos_transformados:
            logger.info("\nüîÑ TRANSFORMACI√ìN:")
            logger.info(f"  ‚Ä¢ Registros finales: {datos_transformados['registros_totales']}")
            logger.info(f"  ‚Ä¢ Columnas: {datos_transformados['columnas_totales']}")
        
        logger.info("=" * 60)
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error en resumen: {str(e)}")
        return False


# ============================================
# CONFIGURACI√ìN DEL DAG
# ============================================

# Argumentos por defecto
default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'start_date': datetime(2025, 10, 26),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# Crear el DAG
with DAG(
    dag_id='pipeline_financiero_etl',
    default_args=default_args,
    description='Pipeline ETL de datos financieros - Extracci√≥n y Transformaci√≥n',
    schedule_interval='@daily',  # Ejecuta una vez al d√≠a
    start_date=datetime(2025, 10, 26),
    catchup=False,  # No ejecutar fechas pasadas
    tags=['etl', 'finance', 'daily'],
    doc_md=__doc__,
) as dag:
    
    # ============================================
    # DEFINICI√ìN DE TAREAS
    # ============================================
    
    # Tarea inicial (dummy)
    inicio = EmptyOperator(
        task_id='inicio',
        doc_md="Marca el inicio del pipeline"
    )
    
    # Tarea 1: Extracci√≥n
    extraer_datos = PythonOperator(
        task_id='extraer_datos',
        python_callable=tarea_extraer_datos,
        doc_md="""
        # Extracci√≥n de Datos
        
        Extrae datos de 3 APIs:
        - API 1: Productos financieros
        - API 2: Tipos de cambio
        - API 3: Datos adicionales
        
        Retorna DataFrames de Pandas con los datos extra√≠dos.
        """
    )
    
    # Tarea 2: Transformaci√≥n
    transformar_datos = PythonOperator(
        task_id='transformar_datos',
        python_callable=tarea_transformar_datos,
        doc_md="""
        # Transformaci√≥n de Datos
        
        - Limpia datos (duplicados, nulos)
        - Consolida 3 fuentes
        - Calcula precio local (USD ‚Üí ARS)
        - Genera resumen estad√≠stico
        """
    )
    
    # Tarea 3: Resumen
    resumen_final = PythonOperator(
        task_id='resumen_final',
        python_callable=tarea_resumen_final,
        doc_md="Genera resumen final de la ejecuci√≥n"
    )
    
    # Tarea final (dummy)
    fin = EmptyOperator(
        task_id='fin',
        doc_md="Marca el fin del pipeline"
    )
    
    # ============================================
    # DEPENDENCIAS (FLUJO DE EJECUCI√ìN)
    # ============================================
    
    inicio >> extraer_datos >> transformar_datos >> resumen_final >> fin

# ============================================
# DOCUMENTACI√ìN DEL DAG
# ============================================

"""
## Pipeline ETL Financiero

### Descripci√≥n
Pipeline diario que extrae datos de APIs financieras, transforma y consolida
la informaci√≥n calculando precios en moneda local.

### Flujo de Ejecuci√≥n
1. **Inicio**: Marca inicio del pipeline
2. **Extracci√≥n**: Extrae de 3 APIs (productos, tipos de cambio, adicionales)
3. **Transformaci√≥n**: Consolida y calcula precios locales
4. **Resumen**: Genera log con estad√≠sticas
5. **Fin**: Marca fin del pipeline

### Pr√≥ximas Mejoras
- D√≠a 8: Agregar carga a staging (PostgreSQL)
- D√≠a 9: Agregar modelado dimensional (SCD Type 2)

### Schedule
- **Frecuencia**: Diaria (@daily)
- **Hora**: 00:00 UTC
- **Catchup**: Desactivado

### Configuraci√≥n
- **Retries**: 2
- **Retry Delay**: 5 minutos
- **Owner**: data_team
"""
