# ğŸ“ˆ Airflow-Finance-SCD2-Pipeline

![CI Pipeline](https://github.com/emanuelferesin/Airflow_Finance_Pipeline/actions/workflows/ci.yml/badge.svg)

Este proyecto implementa un **Pipeline de IngenierÃ­a de Datos ELT** de extremo a extremo, orquestado con **Apache Airflow 2.x** y desplegado usando **Docker Compose**. El objetivo principal es **consolidar datos financieros temporales** de mÃºltiples APIs para construir un Data Warehouse en **PostgreSQL**, utilizando un **Esquema Estrella** avanzado.

---

## ğŸ“Š Estado del Proyecto

âœ… **DÃ­as 1-2**: Setup y Docker Infrastructure  
âœ… **DÃ­as 3-4**: ExtracciÃ³n y TransformaciÃ³n de Datos  
âœ… **DÃ­a 5**: Testing Unitario (7 tests passing)  
âœ… **DÃ­a 6**: CI/CD con GitHub Actions  
ğŸ”„ **PrÃ³ximo**: DAG de Airflow y Modelado Dimensional  

---

## ğŸ”‘ CaracterÃ­sticas Destacadas

* **Arquitectura ELT:** Realiza una carga inicial de datos crudos a una tabla de *staging* y utiliza SQL en PostgreSQL para las transformaciones posteriores (L-T).
* **MÃºltiples Fuentes de Datos Temporales (3 APIs):** Consolida series de tiempo de precios, tipos de cambio y datos adicionales para generar mÃ©tricas derivadas.
* **HistÃ³rico Configurable:** ParÃ¡metro `dias_historico` permite generar datos desde 1 hasta 365 dÃ­as para backfill y testing.
* **Modelado Dimensional Avanzado:** ImplementaciÃ³n de un **Esquema Estrella** que incluye:
    * **Tabla de Hechos:** `fact_ventas` (MÃ©tricas de precios en USD y Moneda Local).
    * **Tabla de DimensiÃ³n:** `dim_producto` (Atributos del producto, usando lÃ³gica **SCD Tipo 2** para rastrear el historial de cambios).
* **IngenierÃ­a de CÃ³digo:** Uso de cÃ³digo Python modular (`src/`) con **Type Hinting** y pruebas unitarias (`pytest`).
* **IntegraciÃ³n Continua (CI):** Flujo de trabajo automatizado con **GitHub Actions** para correr los tests unitarios en cada commit.

---

## ğŸ—ï¸ Arquitectura del Proyecto
```
APIs Externas â†’ Airflow DAG â†’ PostgreSQL (Staging) â†’ TransformaciÃ³n SQL â†’ Data Warehouse
                    â†“
              Testing (pytest)
                    â†“
           CI/CD (GitHub Actions)
```

---

## ğŸš€ Inicio RÃ¡pido

### Prerequisitos
- Docker Desktop
- Docker Compose v3.8+
- Git

### InstalaciÃ³n
```bash
# Clonar repositorio
git clone https://github.com/emanuelferesin/Airflow_Finance_Pipeline.git
cd Airflow_Finance_Pipeline

# Configurar variables de entorno
cp env_example .env

# Dar permisos a carpeta logs
chmod -R 777 logs/

# Iniciar servicios
docker-compose up -d

# Acceder a Airflow UI
# http://localhost:8080
# Usuario: admin
# ContraseÃ±a: admin
```

### Ejecutar Tests
```bash
# Dentro del contenedor
docker-compose exec webserver bash
pytest tests/ -v

# Salir del contenedor
exit
```

---

## ğŸ“ Estructura del Proyecto
```
Airflow_Finance_Pipeline/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml              # CI/CD pipeline
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ __init__.py             # DAGs de Airflow (prÃ³ximamente)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extractor.py            # ExtracciÃ³n de APIs con histÃ³rico
â”‚   â”œâ”€â”€ transformer.py          # TransformaciÃ³n y consolidaciÃ³n
â”‚   â””â”€â”€ utils.py                # Utilidades y manejo de errores
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_extractor.py       # Tests de extracciÃ³n (4 tests)
â”‚   â””â”€â”€ test_transformer.py     # Tests de transformaciÃ³n (3 tests)
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ (prÃ³ximamente)          # Scripts SQL para modelado
â”œâ”€â”€ logs/                       # Logs de Airflow
â”œâ”€â”€ plugins/                    # Plugins personalizados
â”œâ”€â”€ docker-compose.yml          # OrquestaciÃ³n de servicios
â”œâ”€â”€ Dockerfile                  # Imagen personalizada de Airflow
â”œâ”€â”€ requirements.txt            # Dependencias Python
â”œâ”€â”€ pytest.ini                  # ConfiguraciÃ³n de pytest
â””â”€â”€ README.md
```

---

## ğŸ§ª Testing

El proyecto incluye 7 tests unitarios que verifican la funcionalidad core:
```bash
# Ejecutar todos los tests
pytest tests/ -v

# Resultado esperado:
# 7 passed âœ…
```

**Tests implementados:**
- âœ… ExtracciÃ³n de productos funciona correctamente
- âœ… ExtracciÃ³n tiene columnas requeridas
- âœ… ExtracciÃ³n de tipos de cambio funciona
- âœ… Datos de ARS estÃ¡n presentes
- âœ… Limpieza de datos elimina duplicados
- âœ… CÃ¡lculo de precio local es correcto
- âœ… Se agrega columna de moneda local

---

## ğŸ”„ CI/CD

El proyecto utiliza GitHub Actions para:
- âœ… Ejecutar tests automÃ¡ticamente en cada push
- âœ… Verificar calidad de cÃ³digo con flake8
- âœ… Validar instalaciÃ³n de dependencias

**Ver estado:** [![CI Pipeline](https://github.com/emanuelferesin/Airflow_Finance_Pipeline/actions/workflows/ci.yml/badge.svg)](https://github.com/emanuelferesin/Airflow_Finance_Pipeline/actions)

---

## ğŸ’¾ ExtracciÃ³n de Datos

El mÃ³dulo de extracciÃ³n soporta histÃ³rico configurable:
```python
from src.extractor import extraer_todas_las_fuentes

# Extraer 7 dÃ­as de histÃ³rico (default)
datos = extraer_todas_las_fuentes(dias_historico=7)

# Extraer 30 dÃ­as para testing
datos = extraer_todas_las_fuentes(dias_historico=30)

# Para producciÃ³n con Airflow
datos = extraer_todas_las_fuentes(dias_historico=1)
```

**Retorna:**
- DataFrame de productos (~161 productos Ã— N dÃ­as)
- DataFrame de tipos de cambio (~161 monedas Ã— N dÃ­as)
- DataFrame de datos adicionales (~10 registros Ã— N dÃ­as)

---

## ğŸ”„ TransformaciÃ³n de Datos

El mÃ³dulo de transformaciÃ³n consolida y calcula precios locales:
```python
from src.transformer import transformar_datos_completo

# Transformar y consolidar
df_final, resumen = transformar_datos_completo(
    datos['productos'],
    datos['tipos_cambio'],
    datos['adicionales'],
    moneda_local='ARS'
)

# Resultado: DataFrame consolidado con precio_local = precio_usd Ã— tipo_cambio
```

---

## ğŸ“Š Modelo de Datos (PrÃ³ximamente)

### Esquema Estrella

#### Tabla de Hechos: `fact_ventas`
- `venta_id` (PK)
- `producto_key` (FK)
- `fecha_key` (FK)
- `precio_usd`
- `precio_local`
- `tipo_cambio`

#### DimensiÃ³n: `dim_producto` (SCD Type 2)
- `producto_key` (PK - Surrogate Key)
- `producto_id` (Business Key)
- `nombre_producto`
- `categoria`
- `precio_base`
- `fecha_inicio`
- `fecha_fin`
- `es_actual`

---

## ğŸ› ï¸ TecnologÃ­as

- **Apache Airflow 2.7.2** - OrquestaciÃ³n
- **PostgreSQL 13** - Base de datos
- **Python 3.10** - Lenguaje principal
- **Docker & Docker Compose** - ContainerizaciÃ³n
- **Pandas** - ManipulaciÃ³n de datos
- **pytest** - Testing
- **GitHub Actions** - CI/CD

---

## ğŸ“ Comandos Ãštiles
```bash
# Iniciar Airflow
docker-compose up -d

# Ver logs
docker-compose logs -f

# Detener Airflow
docker-compose down

# Limpiar todo (incluye base de datos)
docker-compose down -v

# Ejecutar tests
docker-compose exec webserver pytest tests/ -v

# Acceder al contenedor
docker-compose exec webserver bash
```

---

## ğŸ¤ ContribuciÃ³n

1. Fork el proyecto
2. Crear rama feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -m 'Add nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crear Pull Request

---

## ğŸ“ Licencia

Este proyecto es de cÃ³digo abierto bajo la licencia MIT.

---

## ğŸ‘¤ Autor

Emanuel Feresin - [@emanuelferesin](https://github.com/emanuelferesin)

---

## ğŸ™ Agradecimientos

- Apache Airflow Community
- DocumentaciÃ³n de PostgreSQL
- GitHub Actions
- ExchangeRate API
- Coinbase API

---